#!/usr/bin/python3

import keras , pandas , sklearn
from sklearn import model_selection

#Study
'''
Optimisers:		Adam Gradient Descent
Loss function:		Cross-Entropy (if the labels are integers use sparse_categorical_crossentropy if they are one-hot encoded use categorical_crossentropy)
Activiation:		ReLU for the hidden layers and softmax for the output layer (to give probability of different classes) or linear function (for regression)
			Leaky ReLU (small negative slope rather than 0) is sometimes used to allow better backpropagation if a certain weight have a too large of a negative weight that causes the node to never fire. Only used if large portion of the nodes are deing, other than that always use ReLU.
			Always end with Softmax (for single lable classifying), because it gives probability between 0 and 1
			Always end with Sigmoid (for multi lable classifying), because it does not give probability between 0 and 1
Neural Networks:	Feed Forward (FFNN), Recurrent (RNN & LSTM), Convolutional (CNN), Unsupervised (GAN)
Layers:			*First layer:	1 layer, nodes = ?. The first layer also contains the input layer defined as input_shape = (number of features,) and the ,) indicates that it is a single element tuple
			*Hidden layer:	1 - 2 layers (except for convolutional networks), nodes = ?. The starting number of nodes can be between the number of nodes in the input later and the number of nodes in the output layer. But they can increase to the 100 until good accuracy is acheived
			*Output layer:	1 layer, nodes = number of classes 
Learning Rate:		The steps taken in Gradient Descent to reach the global minima. low = more accurate but slower. If loss is increasing that means the Learning Rate is high. Default is always 0.01
'''

#To run computation on a GPU
'''
#Ubuntu 16.04 GPU Setup:	https://yangcha.github.io/CUDA90/
#Arch Linux:			Install CUDA: sudo pacman -S cuda
#Check to see if Keras on TensorFlow is using the GPU (last line should give the GPU brand)
from keras import backend as K
K.tensorflow_backend._get_available_gpus()
'''

#Tensor Board - sudo pacman -S tensorboard
tensorboard = keras.callbacks.TensorBoard(log_dir = './')
'''
Important parameters to view:
	1. Loss over epoch
	2. Accuracy over epoch

In python add this line:
tensorboard = keras.callbacks.TensorBoard(log_dir = './')
and add callbacks = [tensorboard] to model.fit

In the terminal execute this command, then open the local URL:
tensorboard --logdir=./
'''
#Import data
data = pandas.read_csv('../OLD/MNIST.csv')
X = (data.iloc[:,1:].values) / 255		# Divide each vector value by 255, MinMax regularisation but for each item separatly, therefore 0-255 values become 0-1 values
Y = data.iloc[:,0].values
X , Y = sklearn.utils.shuffle(X , Y , random_state = 0)
n_class = 10
n_featu = X.shape[1]
Y = keras.utils.to_categorical(Y , n_class)	# One-hot encoding
x_train , x_test , y_train , y_test = model_selection.train_test_split(X , Y , random_state = 0)

#Setup neural network
model = keras.models.Sequential()
model.add(keras.layers.core.Dense(30 , input_shape = (n_featu,) , activation = 'relu'))
model.add(keras.layers.core.Dropout(0.2))
model.add(keras.layers.core.Dense(30 , activation = 'relu'))
model.add(keras.layers.core.Dropout(0.2))
model.add(keras.layers.core.Dense(n_class , activation = 'softmax'))

#Compile model
model.compile(keras.optimizers.Adam(lr = 0.001) , loss = 'categorical_crossentropy' , metrics = ['accuracy'])
model.summary()

#Train model
model.fit(x_train , y_train , batch_size = 128 , epochs = 20 , verbose = 2 , validation_data = (x_test, y_test) , callbacks = [tensorboard])

'''
#Save Model
model.save_weights('model.h5')

#Load model and weights
model.load_weights('model.h5')

#Prediction
#prediction = model.predict_classes(numpy.array([[130 , 6.0 , 8.2 , 0.71]]))
print(prediction)

#Evaluate loaded model
model.compile(keras.optimizers.Adam() , loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'])
score = load_model.evaluate(test_x , test_y , verbose = 0)
print('Test accuracy:' , score[1])
'''

#The Shape of data
'''			Dataset Shape
			(5 , 4)						<-- 5 examples each having 4 features
			   |
			   Ù§
			Layer Output Shape	Number of Parameters	<-- Number of trainable wegihts (weights coming from previous layer + 1 bias weight for each node)
input_shape:		(4    ,  )					<-- 4 is the number of features in the dataset for each example
Dense 1:	3 Nodes	(None , 3)		15			<-- Dense layer 1 and the input_shape are coded in the same line in KERAS
Dropout 1:		//			0			<-- Randomly freez 20% of the nodes every epoch to reduce overfitting and get better generalisation
Dense 2:	3 Nodes	(None , 3)		12			<-- None refers to any value in this position , 3 for 3 nodes
Dropout 2:		//			0			<-- Best to use dropout layers between each layer
Dense 3:	2 Nodes	(None , 2)		8			<-- 2 for 2 output nodes (each node for each class)
'''
